import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.util import ngrams
from collections import Counter

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

df = pd.read_csv('train (5).csv')

disaster_tweets = df[df['target'] == 1]
normal_tweets = df[df['target'] == 0]

print("normal" ,normal_tweets)
print("disaster" , disaster_tweets)

def preprocess_tweets(tweets):
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = []
    for tweet in tweets:

        words = word_tokenize(tweet.lower())
        words = [word for word in words if word.isalpha()]
        words = [word for word in words if word not in stopwords.words('english')]

        words = [lemmatizer.lemmatize(word) for word in words]
        tokens.extend(words)
    return tokens

disaster_tokens = preprocess_tweets(disaster_tweets['text'])
print(disaster_tokens)

normal_tokens = preprocess_tweets(normal_tweets['text'])
print(normal_tokens)  
disaster_tokens = preprocess_tweets(disaster_tweets['text'])
normal_tokens = preprocess_tweets(normal_tweets['text'])

disaster_word_count = Counter(disaster_tokens)
print("disaster", disaster_word_count)

normal_word_count = Counter(normal_tokens)
print("normal", normal_word_count)                                                                                                                                               
top_20_disaster_words = disaster_word_count.most_common(20)
top_20_normal_words = normal_word_count.most_common(20)

print("Top 20 disaster words:", top_20_disaster_words)
print("Top 20 normal words:", top_20_normal_words)

def get_top_ngrams(tokens, n=2, top_n=20):
    n_grams = ngrams(tokens, n)
    n_gram_freq = Counter(n_grams)
    return n_gram_freq.most_common(top_n)


top_20_disaster_bigrams = get_top_ngrams(disaster_tokens, n=2, top_n=20)
top_20_disaster_trigrams = get_top_ngrams(disaster_tokens, n=3, top_n=20)
top_20_normal_bigrams = get_top_ngrams(normal_tokens, n=2, top_n=20)
top_20_normal_trigrams = get_top_ngrams(normal_tokens, n=3, top_n=20)


print("Top 20 disaster bigrams:", top_20_disaster_bigrams)
print("Top 20 disaster trigrams:", top_20_disaster_trigrams)
print("Top 20 normal bigrams:", top_20_normal_bigrams)
print("Top 20 normal trigrams:", top_20_normal_trigrams)

#Task B
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from string import punctuation

nltk.download('stopwords')

df = pd.read_csv('train (5).csv')

disaster_tweets = df[df['target'] == 1]
normal_tweets = df[df['target'] == 0]

def preprocess_tweet(tweet):
    tweet = re.sub(r'@\w+', '', tweet)
    tweet = ''.join([char for char in tweet if char not in punctuation])
    tweet = tweet.lower()
    stop_words = set(stopwords.words('english'))
    tweet = ' '.join([word for word in tweet.split() if word not in stop_words])
    return tweet


disaster_tweets['text'] = disaster_tweets['text'].apply(preprocess_tweet)
normal_tweets['text'] = normal_tweets['text'].apply(preprocess_tweet)

print("Disaster tweets:", disaster_tweets['text'])
print("Normal tweets:", normal_tweets['text'])
# Task C
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, recall_score

train_df = pd.read_csv('train.csv')


X_train, X_val, y_train, y_val = train_test_split(train_df['review'], train_df['sentiment'], test_size=0.2, random_state=42)

def train_and_evaluate(max_features):
    # Vectorizing text data
    vectorizer = CountVectorizer(max_features=max_features)
    X_train_vect = vectorizer.fit_transform(X_train)
    X_val_vect = vectorizer.transform(X_val)

   
    lr_model = LogisticRegression(max_iter=1000)
    lr_model.fit(X_train_vect, y_train)

    y_pred = lr_model.predict(X_val_vect)

    accuracy = accuracy_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred, pos_label='positive')
    recall = recall_score(y_val, y_pred, pos_label='positive')

    return accuracy, f1, recall

results_100 = train_and_evaluate(100)
results_1000 = train_and_evaluate(1000)

print("results_100" ,results_100)
print("results_1000" , results_1000)
